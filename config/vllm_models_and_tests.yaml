# ---------------------------------------------------------------------------
# PLANT DOC: image-to-text models
# ---------------------------------------------------------------------------

# ---------------------------
# Gemma3 27B Instruct
# ---------------------------
gemma3_27b_bf16_1node_2gpu:
  model_id: "google/gemma-3-27b-it"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_chat_template"
  use_chat_template: true
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 2
    max_model_len: 8192
  sampling_params:
    temperature: 0.6
    max_tokens: 250

gemma3_27b_bf16_1node_4gpu:
  model_id: "google/gemma-3-27b-it"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_chat_template"
  use_chat_template: true
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 4
    max_model_len: 8192
  sampling_params:
    temperature: 0.6
    max_tokens: 250

# ---------------------------
# Qwen 2.5 VL 32B Instruct
# ---------------------------
qwen2_5vl_32b_bf16_1node_4gpu:
  model_id: "Qwen/Qwen2.5-VL-32B-Instruct"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_chat_template"
  use_chat_template: true
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 4
    max_model_len: 8192
  sampling_params:
    temperature: 0.6
    max_tokens: 250

# ---------------------------
# Mistral Small 3.1 24B Instruct
# ---------------------------
mistral_small31_24b_bf16_1node_2gpu:
  model_id: "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_chat_template"
  use_chat_template: true
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 2
    max_model_len: 8192
  sampling_params:
    temperature: 0.15
    max_tokens: 250

mistral_small31_24b_bf16_1node_4gpu:
  model_id: "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_chat_template"
  use_chat_template: true
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 4
    max_model_len: 8192
  sampling_params:
    temperature: 0.15
    max_tokens: 250

# ---------------------------
# Phi 3.5 Vision 4.2B Instruct
# ---------------------------
phi3_5_vision_bf16_1node_2gpu:
  model_id: "microsoft/Phi-3.5-vision-instruct"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_vlm_phi3_vision_template"
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 2
    max_model_len: 8192
  sampling_params:
    temperature: 0.1
    max_tokens: 250

phi3_5_vision_bf16_1node_4gpu:
  model_id: "microsoft/Phi-3.5-vision-instruct"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_vlm_phi3_vision_template"
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 4
    max_model_len: 8192
  sampling_params:
    temperature: 0.1
    max_tokens: 250

# ---------------------------
# SmolVLM2 2.2B Instruct
# ---------------------------
smolvlm2_2_2b_bf16_1node_2gpu:
  model_id: "HuggingFaceTB/SmolVLM2-2.2B-Instruct"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_chat_template"
  use_chat_template: true
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 2
    max_model_len: 8192
  sampling_params:
    temperature: 0.6
    max_tokens: 250

smolvlm2_2_2b_bf16_1node_4gpu:
  model_id: "HuggingFaceTB/SmolVLM2-2.2B-Instruct"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_chat_template"
  use_chat_template: true
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 4
    max_model_len: 8192
  sampling_params:
    temperature: 0.6
    max_tokens: 250

# ---------------------------
# InternVL3 38B
# ---------------------------
internvl3_38b_bf16_1node_4gpu:
  model_id: "OpenGVLab/InternVL3-38B"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_vlm"
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 4
    max_model_len: 8192
  sampling_params:
    temperature: 0.6
    max_tokens: 250

# ---------------------------
# InternVL3 14B Instruct
# ---------------------------
internvl3_14b_hf_bf16_1node_2gpu:
  model_id: "OpenGVLab/InternVL3-14B-Instruct"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_vlm"
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 2
    max_model_len: 8192
  sampling_params:
    temperature: 0.6
    max_tokens: 250

internvl3_14b_hf_bf16_1node_4gpu:
  model_id: "OpenGVLab/InternVL3-14B-Instruct"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_vlm"
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 4
    max_model_len: 8192
  sampling_params:
    temperature: 0.6
    max_tokens: 250

# ---------------------------
# Llava 1.5 7B
# ---------------------------
llava_fp16_1node_1gpu:
  model_id: "llava-hf/llava-1.5-7b-hf"
  task_type: "image-to-text" 
  prompt_key: "plant_doc_description_vlm"
  vllm_engine_args:
    dtype: "float16"
    trust_remote_code: True
    tensor_parallel_size: 1
    max_model_len: 4096
  sampling_params:
    temperature: 0.6
    max_tokens: 250

llava_bf16_1node_2gpu:
  model_id: "llava-hf/llava-1.5-7b-hf"
  task_type: "image-to-text" 
  prompt_key: "plant_doc_description_vlm"
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 2
    max_model_len: 4096
  sampling_params:
    temperature: 0.6
    max_tokens: 250

llava_bf16_1node_4gpu:
  model_id: "llava-hf/llava-1.5-7b-hf"
  task_type: "image-to-text"
  prompt_key: "plant_doc_description_vlm"
  vllm_engine_args:
    dtype: "bfloat16"
    trust_remote_code: True
    tensor_parallel_size: 4
    max_model_len: 4096
  sampling_params:
    temperature: 0.6
    max_tokens: 250

# ---------------------------------------------------------------------------
# PLANT DOC: text-to-text models
# ---------------------------------------------------------------------------
llama3_2_1b_instruct_fp16_1node_1gpu_summarize: 
  model_id: "meta-llama/Llama-3.2-1B-Instruct"
  task_type: "text-to-text"
  prompt_key: "summarize_plant_description_llama_template_short"
  input_description_source_key: "llava_fp16_1node_1gpu" 
  vllm_engine_args:
    dtype: "bfloat16" 
    trust_remote_code: True
    tensor_parallel_size: 1
  sampling_params:
    temperature: 0.6
    max_tokens: 250
